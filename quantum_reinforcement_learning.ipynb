{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kWo-GNlwpT6"
   },
   "source": [
    "##### Copyright 2021 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5w2rucWZwpUA"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHebkma_wpUC"
   },
   "source": [
    "# Parametrized Quantum Circuits for Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQf8eEUewpUD"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/quantum/tutorials/quantum_reinforcement_learning\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/quantum/blob/master/docs/tutorials/quantum_reinforcement_learning.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/quantum/blob/master/docs/tutorials/quantum_reinforcement_learning.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/quantum/docs/tutorials/quantum_reinforcement_learning.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NJTIUX_wpUE"
   },
   "source": [
    "Quantum computers have been shown to provide computational advantages in certain problem areas. The field of quantum reinforcement learning (QRL) aims to harness this boost by designing RL agents that rely on quantum models of computation.\n",
    "\n",
    "In this tutorial, you will implement two reinforcement learning algorithms based on parametrized/variational quantum circuits (PQCs or VQCs), namely a policy-gradient and a deep Q-learning implementation. These algorithms were introduced by [[1] Jerbi et al.](https://arxiv.org/abs/2103.05577) and [[2] Skolik et al.](https://arxiv.org/abs/2103.15084), respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYkcUIu4wpUF"
   },
   "source": [
    "You will implement a PQC with data re-uploading in TFQ, and use it as:\n",
    "1. an RL policy trained with a policy-gradient method,\n",
    "2. a Q-function approximator trained with deep Q-learning,\n",
    "\n",
    "each solving [CartPole-v1](http://gym.openai.com/envs/CartPole-v1/), a benchmarking task from OpenAI Gym. Note that, as showcased in [[1]](https://arxiv.org/abs/2103.05577) and [[2]](https://arxiv.org/abs/2103.15084), these agents can also be used to solve other task-environment from OpenAI Gym, such as [FrozenLake-v0](http://gym.openai.com/envs/FrozenLake-v0/), [MountainCar-v0](http://gym.openai.com/envs/MountainCar-v0/) or [Acrobot-v1](http://gym.openai.com/envs/Acrobot-v1/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gw0D-uwmwpUF"
   },
   "source": [
    "Features of this implementation:\n",
    "- you will learn how to use a `tfq.layers.ControlledPQC` to implement a PQC with data re-uploading, appearing in many applications of QML. This implementation also naturally allows using trainable scaling parameters at the input of the PQC, to increase its expressivity,\n",
    "- you will learn how to implement observables with trainable weights at the output of a PQC, to allow a flexible range of output values,\n",
    "- you will learn how a `tf.keras.Model` can be trained with non-trivial ML loss functions, i.e., that are not compatible with `model.compile` and `model.fit`, using a `tf.GradientTape`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLSoeBdTwpUF"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Id8vB7FiwpUJ"
   },
   "source": [
    "Now import TensorFlow and the module dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Ql5PW-ACO0J"
   },
   "outputs": [],
   "source": [
    "# Update package resources to account for version changes.\n",
    "import importlib, pkg_resources\n",
    "importlib.reload(pkg_resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIIYRJ79wpUK"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_quantum as tfq\n",
    "\n",
    "import gym, cirq, sympy\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from collections import deque, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from cirq.contrib.svg import SVGCircuit\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxWGru_NwpUK"
   },
   "source": [
    "## 1. Build a PQC with data re-uploading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85woLQQswpUL"
   },
   "source": [
    "At the core of both RL algorithms you are implementing is a PQC that takes as input the agent's state $s$ in the environment (i.e., a numpy array) and outputs a vector of expectation values. These expectation values are then post-processed, either to produce an agent's policy $\\pi(a|s)$ or approximate Q-values $Q(s,a)$. In this way, the PQCs are playing an analog role to that of deep neural networks in modern deep RL algorithms.\n",
    "\n",
    "A popular way to encode an input vector in a PQC is through the use of single-qubit rotations, where rotation angles are controlled by the components of this input vector. In order to get a [highly-expressive model](https://arxiv.org/abs/2008.08605), these single-qubit encodings are not performed only once in the PQC, but in several \"[re-uploadings](https://quantum-journal.org/papers/q-2020-02-06-226/)\", interlayed with variational gates. The layout of such a PQC is depicted below:\n",
    "\n",
    "<img src=\"./images/pqc_re-uploading.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxw3Rz0awpUL"
   },
   "source": [
    "As discussed in [[1]](https://arxiv.org/abs/2103.05577) and [[2]](https://arxiv.org/abs/2103.15084), a way to further enhance the expressivity and trainability of data re-uploading PQCs is to use trainable input-scaling parameters $\\boldsymbol{\\lambda}$ for each encoding gate of the PQC, and trainable observable weights $\\boldsymbol{w}$ at its output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNSjI-OywpUM"
   },
   "source": [
    "### 1.1 Cirq circuit for ControlledPQC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCYrUUwswpUM"
   },
   "source": [
    "The first step is to implement in Cirq the quantum circuit to be used as the PQC. For this, start by defining basic unitaries to be applied in the circuits, namely an arbitrary single-qubit rotation and an entangling layer of CZ gates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4P5EORYwpUM"
   },
   "outputs": [],
   "source": [
    "def one_qubit_rotation(qubit, symbols):\n",
    "    \"\"\"\n",
    "    Returns Cirq gates that apply a rotation of the bloch sphere about the X,\n",
    "    Y and Z axis, specified by the values in `symbols`.\n",
    "    \"\"\"\n",
    "    return [cirq.rx(symbols[0])(qubit),\n",
    "            cirq.ry(symbols[1])(qubit),\n",
    "            cirq.rz(symbols[2])(qubit)]\n",
    "\n",
    "def entangling_layer(qubits):\n",
    "    \"\"\"\n",
    "    Returns a layer of CZ entangling gates on `qubits` (arranged in a circular topology).\n",
    "    \"\"\"\n",
    "    cz_ops = [cirq.CZ(q0, q1) for q0, q1 in zip(qubits, qubits[1:])]\n",
    "    cz_ops += ([cirq.CZ(qubits[0], qubits[-1])] if len(qubits) != 2 else [])\n",
    "    return cz_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTgpkm6iwpUM"
   },
   "source": [
    "Now, use these functions to generate the Cirq circuit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEicpzq9wpUN"
   },
   "outputs": [],
   "source": [
    "def generate_circuit(qubits, n_layers):\n",
    "    \"\"\"Prepares a data re-uploading circuit on `qubits` with `n_layers` layers.\"\"\"\n",
    "    # Number of qubits\n",
    "    n_qubits = len(qubits)\n",
    "    \n",
    "    # Sympy symbols for variational angles\n",
    "    params = sympy.symbols(f'theta(0:{3*(n_layers+1)*n_qubits})')\n",
    "    params = np.asarray(params).reshape((n_layers + 1, n_qubits, 3))\n",
    "    \n",
    "    # Sympy symbols for encoding angles\n",
    "    inputs = sympy.symbols(f'x(0:{n_layers})'+f'_(0:{n_qubits})')\n",
    "    inputs = np.asarray(inputs).reshape((n_layers, n_qubits))\n",
    "    \n",
    "    # Define circuit\n",
    "    circuit = cirq.Circuit()\n",
    "    for l in range(n_layers):\n",
    "        # Variational layer\n",
    "        circuit += cirq.Circuit(one_qubit_rotation(q, params[l, i]) for i, q in enumerate(qubits))\n",
    "        circuit += entangling_layer(qubits)\n",
    "        # Encoding layer\n",
    "        circuit += cirq.Circuit(cirq.rx(inputs[l, i])(q) for i, q in enumerate(qubits))\n",
    "\n",
    "    # Last varitional layer\n",
    "    circuit += cirq.Circuit(one_qubit_rotation(q, params[n_layers, i]) for i,q in enumerate(qubits))\n",
    "    \n",
    "    return circuit, list(params.flat), list(inputs.flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZL8MvT21wpUN"
   },
   "source": [
    "Check that this produces a circuit that is alternating between variational and encoding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "id": "M4LFL2bQwpUO",
    "outputId": "e446c0c0-f844-4d63-9f29-d01f8aeed411"
   },
   "outputs": [],
   "source": [
    "n_qubits, n_layers = 4, 1\n",
    "qubits = cirq.GridQubit.rect(1, n_qubits)\n",
    "circuit, _, _ = generate_circuit(qubits, n_layers)\n",
    "SVGCircuit(circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RrFUkT3wpUP"
   },
   "source": [
    "### 1.2 ReUploadingPQC layer using ControlledPQC\n",
    "\n",
    "To construct the re-uploading PQC from the figure above, you can create a custom Keras layer. This layer will manage the trainable parameters (variational angles $\\boldsymbol{\\theta}$ and input-scaling parameters $\\boldsymbol{\\lambda}$) and resolve the input values (input state $s$) into the appropriate symbols in the circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7XJvWgQ4wpUP"
   },
   "outputs": [],
   "source": [
    "class ReUploadingPQC(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Performs the transformation (s_1, ..., s_d) -> (theta_1, ..., theta_N, lmbd[1][1]s_1, ..., lmbd[1][M]s_1,\n",
    "        ......., lmbd[d][1]s_d, ..., lmbd[d][M]s_d) for d=input_dim, N=theta_dim and M=n_layers.\n",
    "    An activation function from tf.keras.activations, specified by `activation` ('linear' by default) is\n",
    "        then applied to all lmbd[i][j]s_i.\n",
    "    All angles are finally permuted to follow the alphabetical order of their symbol names, as processed\n",
    "        by the ControlledPQC.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, qubits, n_layers, observables, activation=\"linear\", name=\"re-uploading_PQC\"):\n",
    "        super(ReUploadingPQC, self).__init__(name=name)\n",
    "        self.n_layers = n_layers\n",
    "        self.n_qubits = len(qubits)\n",
    "\n",
    "        circuit, theta_symbols, input_symbols = generate_circuit(qubits, n_layers)\n",
    "\n",
    "        theta_init = tf.random_uniform_initializer(minval=0.0, maxval=np.pi)\n",
    "        self.theta = tf.Variable(\n",
    "            initial_value=theta_init(shape=(1, len(theta_symbols)), dtype=\"float32\"),\n",
    "            trainable=True, name=\"thetas\"\n",
    "        )\n",
    "        \n",
    "        lmbd_init = tf.ones(shape=(self.n_qubits * self.n_layers,))\n",
    "        self.lmbd = tf.Variable(\n",
    "            initial_value=lmbd_init, dtype=\"float32\", trainable=True, name=\"lambdas\"\n",
    "        )\n",
    "        \n",
    "        # Define explicit symbol order.\n",
    "        symbols = [str(symb) for symb in theta_symbols + input_symbols]\n",
    "        self.indices = tf.constant([symbols.index(a) for a in sorted(symbols)])\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.empty_circuit = tfq.convert_to_tensor([cirq.Circuit()])\n",
    "        self.computation_layer = tfq.layers.ControlledPQC(circuit, observables)        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs[0] = encoding data for the state.\n",
    "        batch_dim = tf.gather(tf.shape(inputs[0]), 0)\n",
    "        tiled_up_circuits = tf.repeat(self.empty_circuit, repeats=batch_dim)\n",
    "        tiled_up_thetas = tf.tile(self.theta, multiples=[batch_dim, 1])\n",
    "        tiled_up_inputs = tf.tile(inputs[0], multiples=[1, self.n_layers])\n",
    "        scaled_inputs = tf.einsum(\"i,ji->ji\", self.lmbd, tiled_up_inputs)\n",
    "        squashed_inputs = tf.keras.layers.Activation(self.activation)(scaled_inputs)\n",
    "\n",
    "        joined_vars = tf.concat([tiled_up_thetas, squashed_inputs], axis=1)\n",
    "        joined_vars = tf.gather(joined_vars, self.indices, axis=1)\n",
    "        \n",
    "        return self.computation_layer([tiled_up_circuits, joined_vars])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_u3QBKbvwpUP"
   },
   "source": [
    "## 2. Policy-gradient RL with PQC policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4deMRl86wpUP"
   },
   "source": [
    "In this section, you will implement the policy-gradient algorithm presented in <a href=\"https://arxiv.org/abs/2103.05577\" class=\"external\">[1]</a>. For this, you will start by constructing, out of the PQC that was just defined, the `softmax-VQC` policy (where VQC stands for variational quantum circuit):\n",
    "$$ \\pi_\\theta(a|s) = \\frac{e^{\\beta \\langle O_a \\rangle_{s,\\theta}}}{\\sum_{a'} e^{\\beta \\langle O_{a'} \\rangle_{s,\\theta}}} $$\n",
    "where $\\langle O_a \\rangle_{s,\\theta}$ are expectation values of observables $O_a$ (one per action) measured at the output of the PQC, and $\\beta$ is a tunable inverse-temperature parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wb7zQF5AwpUQ"
   },
   "source": [
    "You can adopt the same observables used in <a href=\"https://arxiv.org/abs/2103.05577\" class=\"external\">[1]</a> for CartPole, namely a global $Z_0Z_1Z_2Z_3$ Pauli product acting on all qubits, weighted by an action-specific weight for each action. To implement the weighting of the Pauli product, you can use an extra `tf.keras.layers.Layer` that stores the action-specific weights and applies them multiplicatively on the expectation value $\\langle Z_0Z_1Z_2Z_3 \\rangle_{s,\\theta}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPLHsGRewpUQ"
   },
   "outputs": [],
   "source": [
    "class Alternating(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_dim):\n",
    "        super(Alternating, self).__init__()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=tf.constant([[(-1.)**i for i in range(output_dim)]]), dtype=\"float32\",\n",
    "            trainable=True, name=\"obs-weights\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdyTMNPTwpUQ"
   },
   "source": [
    "Prepare the definition of your PQC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3yZCMhywpUQ"
   },
   "outputs": [],
   "source": [
    "n_qubits = 4 # Dimension of the state vectors in CartPole\n",
    "n_layers = 5 # Number of layers in the PQC\n",
    "n_actions = 2 # Number of actions in CartPole\n",
    "\n",
    "qubits = cirq.GridQubit.rect(1, n_qubits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMGNUCmOwpUR"
   },
   "source": [
    "and its observables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMAc2_--wpUR"
   },
   "outputs": [],
   "source": [
    "ops = [cirq.Z(q) for q in qubits]\n",
    "observables = [reduce((lambda x, y: x * y), ops)] # Z_0*Z_1*Z_2*Z_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "px9D6vE8wpUR"
   },
   "source": [
    "With this, define a `tf.keras.Model` that applies, sequentially, the `ReUploadingPQC` layer previously defined, followed by a post-processing layer that computes the weighted observables using `Alternating`, which are then fed into a `tf.keras.layers.Softmax` layer that outputs the `softmax-VQC` policy of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ivAvce6wpUR"
   },
   "outputs": [],
   "source": [
    "def generate_model_policy(qubits, n_layers, n_actions, beta, observables):\n",
    "    \"\"\"Generates a Keras model for a data re-uploading PQC policy.\"\"\"\n",
    "\n",
    "    input_tensor = tf.keras.Input(shape=(len(qubits), ), dtype=tf.dtypes.float32, name='input')\n",
    "    re_uploading_pqc = ReUploadingPQC(qubits, n_layers, observables)([input_tensor])\n",
    "    process = tf.keras.Sequential([\n",
    "        Alternating(n_actions),\n",
    "        tf.keras.layers.Lambda(lambda x: x * beta),\n",
    "        tf.keras.layers.Softmax()\n",
    "    ], name=\"observables-policy\")\n",
    "    policy = process(re_uploading_pqc)\n",
    "    model = tf.keras.Model(inputs=[input_tensor], outputs=policy)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = generate_model_policy(qubits, n_layers, n_actions, 1.0, observables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "ANysIOrswpUS",
    "outputId": "131f0db1-8525-4cfa-a6c5-2b0c8573bdb1"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ec-s2ECYwpUS"
   },
   "source": [
    "You can now train the PQC policy on CartPole-v1, using, e.g., the basic `REINFORCE` algorithm (see Alg. 1 in <a href=\"https://arxiv.org/abs/2103.05577\" class=\"external\">[1]</a>). Pay attention to the following points:\n",
    "1. Because scaling parameters, variational angles and observables weights are trained with different learning rates, it is convenient to define 3 separate optimizers with their own learning rates, each updating one of these groups of parameters.\n",
    "2. The loss function in policy-gradient RL is\n",
    "    $$ \\mathcal{L}(\\theta) = -\\frac{1}{|\\mathcal{B}|}\\sum_{s_0,a_0,r_1,s_1,a_1, \\ldots \\in \\mathcal{B}} \\left(\\sum_{t=0}^{H-1} \\log(\\pi_\\theta(a_t|s_t)) \\sum_{t'=1}^{H-t} \\gamma^{t'} r_{t+t'} \\right)$$\n",
    "for a batch $\\mathcal{B}$ of episodes $(s_0,a_0,r_1,s_1,a_1, \\ldots)$ of interactions in the environment following the policy $\\pi_\\theta$. This is different from a supervised learning loss with fixed target values that the model should fit, which make it impossible to use a simple function call like `model.fit` to train the policy. Instead, using a `tf.GradientTape` allows to keep track of the computations involving the PQC (i.e., policy sampling) and store their contributions to the loss during the interaction. After running a batch of episodes, you can then apply backpropagation on these computations to get the gradients of the loss with respect to the PQC parameters and use the optimizers to update the policy-model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHS7UlTHwpUS"
   },
   "source": [
    "Start by defining a function that gathers episodes of interaction with the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYepv83JwpUT"
   },
   "outputs": [],
   "source": [
    "def gather_episodes(state_bounds, n_actions, model, n_episodes, env_name):\n",
    "    \"\"\"Interact with environment in batched fashion.\"\"\"\n",
    "\n",
    "    trajectories = [defaultdict(list) for _ in range(n_episodes)]\n",
    "    envs = [gym.make(env_name) for _ in range(n_episodes)]\n",
    "\n",
    "    done = [False for _ in range(n_episodes)]\n",
    "    states = [e.reset()[0] for e in envs]\n",
    "\n",
    "    while not all(done):\n",
    "        unfinished_ids = [i for i in range(n_episodes) if not done[i]]\n",
    "        normalized_states = [s/state_bounds for i, s in enumerate(states) if not done[i]]\n",
    "\n",
    "        for i, state in zip(unfinished_ids, normalized_states):\n",
    "            trajectories[i]['states'].append(state)\n",
    "\n",
    "        # Compute policy for all unfinished envs in parallel\n",
    "        states = tf.convert_to_tensor(normalized_states)\n",
    "        action_probs = model([states])\n",
    "\n",
    "        # Store action and transition all environments to the next state\n",
    "        states = [None for i in range(n_episodes)]\n",
    "        for i, policy in zip(unfinished_ids, action_probs.numpy()):\n",
    "            action = np.random.choice(n_actions, p=policy)\n",
    "            states[i], reward, terminated, truncated, _ = envs[i].step(action)\n",
    "            done[i] = terminated or truncated\n",
    "            trajectories[i]['actions'].append(action)\n",
    "            trajectories[i]['rewards'].append(reward)\n",
    "\n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJRGD1g1wpUT"
   },
   "source": [
    "and a function that computes discounted returns $\\sum_{t'=1}^{H-t} \\gamma^{t'} r_{t+t'}$ out of the rewards $r_t$ collected in an episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGDLrNN1wpUT"
   },
   "outputs": [],
   "source": [
    "def compute_returns(rewards_history, gamma):\n",
    "    \"\"\"Compute discounted returns with discount factor `gamma`.\"\"\"\n",
    "    returns = []\n",
    "    discounted_sum = 0\n",
    "    for r in rewards_history[::-1]:\n",
    "        discounted_sum = r + gamma * discounted_sum\n",
    "        returns.insert(0, discounted_sum)\n",
    "\n",
    "    # Normalize them for faster and more stable learning\n",
    "    returns = np.array(returns)\n",
    "    returns = (returns - np.mean(returns)) / (np.std(returns) + 1e-8)\n",
    "    returns = returns.tolist()\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkuUMdskwpUT"
   },
   "source": [
    "Define the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUuSU1LRwpUU"
   },
   "outputs": [],
   "source": [
    "state_bounds = np.array([2.4, 2.5, 0.21, 2.5])\n",
    "gamma = 1\n",
    "batch_size = 10\n",
    "n_episodes = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PM8uFSLMwpUU"
   },
   "source": [
    "Prepare the optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fxGvCKpwpUU"
   },
   "outputs": [],
   "source": [
    "optimizer_in = tf.keras.optimizers.Adam(learning_rate=0.1, amsgrad=True)\n",
    "optimizer_var = tf.keras.optimizers.Adam(learning_rate=0.01, amsgrad=True)\n",
    "optimizer_out = tf.keras.optimizers.Adam(learning_rate=0.1, amsgrad=True)\n",
    "\n",
    "# Assign the model parameters to each optimizer\n",
    "w_in, w_var, w_out = 1, 0, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbVHz19-wpUU"
   },
   "source": [
    "Implement a function that updates the policy using states, actions and returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLfbu8Q2wpUV"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def reinforce_update(states, actions, returns, model):\n",
    "    states = tf.convert_to_tensor(states)\n",
    "    actions = tf.convert_to_tensor(actions)\n",
    "    returns = tf.convert_to_tensor(returns)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        logits = model(states)\n",
    "        p_actions = tf.gather_nd(logits, actions)\n",
    "        log_probs = tf.math.log(p_actions)\n",
    "        loss = tf.math.reduce_sum(-log_probs * returns) / batch_size\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    for optimizer, w in zip([optimizer_in, optimizer_var, optimizer_out], [w_in, w_var, w_out]):\n",
    "        optimizer.apply_gradients([(grads[w], model.trainable_variables[w])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrPlDlqLwpUV"
   },
   "source": [
    "Now implement the main training loop of the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95Foz1XewpUV"
   },
   "source": [
    "Note: This agent may need to simulate several million quantum circuits and can take as much as ~20 minutes to finish training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cYSDSNGlwpUW",
    "outputId": "ae603811-b930-4484-a002-d26c4673fa06"
   },
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "\n",
    "# Start training the agent\n",
    "episode_reward_history = []\n",
    "for batch in range(n_episodes // batch_size):\n",
    "    # Gather episodes\n",
    "    episodes = gather_episodes(state_bounds, n_actions, model, batch_size, env_name)\n",
    "    \n",
    "    # Group states, actions and returns in numpy arrays\n",
    "    states = np.concatenate([ep['states'] for ep in episodes])\n",
    "    actions = np.concatenate([ep['actions'] for ep in episodes])\n",
    "    rewards = [ep['rewards'] for ep in episodes]\n",
    "    returns = np.concatenate([compute_returns(ep_rwds, gamma) for ep_rwds in rewards])\n",
    "    returns = np.array(returns, dtype=np.float32)\n",
    "\n",
    "    id_action_pairs = np.array([[i, a] for i, a in enumerate(actions)])\n",
    "    \n",
    "    # Update model parameters.\n",
    "    reinforce_update(states, id_action_pairs, returns, model)\n",
    "\n",
    "    # Store collected rewards\n",
    "    for ep_rwds in rewards:\n",
    "        episode_reward_history.append(np.sum(ep_rwds))\n",
    "        \n",
    "    avg_rewards = np.mean(episode_reward_history[-10:])\n",
    "\n",
    "    print('Finished episode', (batch + 1) * batch_size,\n",
    "          'Average rewards: ', avg_rewards)\n",
    "    \n",
    "    if avg_rewards >= 500.0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8E7Be2SqwpUW"
   },
   "source": [
    "Plot the learning history of the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "51RzNBZqwpUX",
    "outputId": "36b2eae1-7113-4d21-f39e-7d1bab291401",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(episode_reward_history)\n",
    "plt.xlabel('Epsiode')\n",
    "plt.ylabel('Collected rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qfdf4DBkwpUX"
   },
   "source": [
    "Congratulations, you have trained a quantum policy gradient model on Cartpole! The plot above shows the rewards collected by the agent per episode throughout its interaction with the environment. You should see that after a few hundred episodes, the performance of the agent gets close to optimal, i.e., 500 rewards per episode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtaBfoERwpUX"
   },
   "source": [
    "You can now visualize the performance of your agent using `env.render()` in a sample episode (uncomment/run the following cell only if your notebook has access to a display):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-VpROTJ1wpUX"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "state, _ = env.reset()\n",
    "frames = []\n",
    "for t in range(500):\n",
    "    im = Image.fromarray(env.render())\n",
    "    frames.append(im)\n",
    "    policy = model([tf.convert_to_tensor([state/state_bounds])])\n",
    "    action = np.random.choice(n_actions, p=policy.numpy()[0])\n",
    "    state, _, terminated, truncated, _ = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "env.close()\n",
    "frames[1].save('./images/gym_CartPole.gif',\n",
    "               save_all=True, append_images=frames[2:], optimize=False, duration=40, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0iA0nubwpUX"
   },
   "source": [
    "<img src=\"./images/gym_CartPole.gif\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAO1TBxqwpUX"
   },
   "source": [
    "## 3. Deep Q-learning with PQC Q-function approximators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uEimdpHwpUY"
   },
   "source": [
    "In this section, you will move to the implementation of the deep Q-learning algorithm presented in <a href=\"https://arxiv.org/abs/2103.15084\" class=\"external\">[2]</a>. As opposed to a policy-gradient approach, the deep Q-learning method uses a PQC to approximate the Q-function of the agent. That is, the PQC defines a function approximator:\n",
    "$$ Q_\\theta(s,a) = \\langle O_a \\rangle_{s,\\theta} $$\n",
    "where $\\langle O_a \\rangle_{s,\\theta}$ are expectation values of observables $O_a$ (one per action) measured at the ouput of the PQC.\n",
    "\n",
    "These Q-values are updated using a loss function derived from Q-learning:\n",
    "$$ \\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{s,a,r,s' \\in \\mathcal{B}} \\left(Q_\\theta(s,a) - [r +\\max_{a'} Q_{\\theta'}(s',a')]\\right)^2$$\n",
    "for a batch $\\mathcal{B}$ of $1$-step interactions $(s,a,r,s')$ with the environment, sampled from the replay memory, and parameters $\\theta'$ specifying the target PQC (i.e., a copy of the main PQC, whose parameters are sporadically copied from the main PQC throughout learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTyRzuDYwpUY"
   },
   "source": [
    "You can adopt the same observables used in <a href=\"https://arxiv.org/abs/2103.15084\" class=\"external\">[2]</a> for CartPole, namely a $Z_0Z_1$ Pauli product for action $0$ and a $Z_2Z_3$ Pauli product for action $1$. Both observables are re-scaled so their expectation values are in $[0,1]$ and weighted by an action-specific weight. To implement the re-scaling and weighting of the Pauli products, you can define again an extra `tf.keras.layers.Layer` that stores the action-specific weights and applies them multiplicatively on the expectation values $\\left(1+\\langle Z_0Z_1 \\rangle_{s,\\theta}\\right)/2$ and $\\left(1+\\langle Z_2Z_3 \\rangle_{s,\\theta}\\right)/2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MX5l96qywpUY"
   },
   "outputs": [],
   "source": [
    "class Rescaling(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Rescaling, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=tf.ones(shape=(1,input_dim)), dtype=\"float32\",\n",
    "            trainable=True, name=\"obs-weights\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.math.multiply((inputs+1)/2, tf.repeat(self.w,repeats=tf.shape(inputs)[0],axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oesnEQa7wpUY"
   },
   "source": [
    "Prepare the definition of your PQC and its observables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpV0PxZqwpUY"
   },
   "outputs": [],
   "source": [
    "n_qubits = 4 # Dimension of the state vectors in CartPole\n",
    "n_layers = 5 # Number of layers in the PQC\n",
    "n_actions = 2 # Number of actions in CartPole\n",
    "\n",
    "qubits = cirq.GridQubit.rect(1, n_qubits)\n",
    "ops = [cirq.Z(q) for q in qubits]\n",
    "observables = [ops[0]*ops[1], ops[2]*ops[3]] # Z_0*Z_1 for action 0 and Z_2*Z_3 for action 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLMvQBXFwpUZ"
   },
   "source": [
    "Define a `tf.keras.Model` that, similarly to the PQC-policy model, constructs a Q-function approximator that is used to generate the main and target models of our Q-learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBGM6RHIwpUZ"
   },
   "outputs": [],
   "source": [
    "def generate_model_Qlearning(qubits, n_layers, n_actions, observables, target):\n",
    "    \"\"\"Generates a Keras model for a data re-uploading PQC Q-function approximator.\"\"\"\n",
    "\n",
    "    input_tensor = tf.keras.Input(shape=(len(qubits), ), dtype=tf.dtypes.float32, name='input')\n",
    "    re_uploading_pqc = ReUploadingPQC(qubits, n_layers, observables, activation='tanh')([input_tensor])\n",
    "    process = tf.keras.Sequential([Rescaling(len(observables))], name=target*\"Target\"+\"Q-values\")\n",
    "    Q_values = process(re_uploading_pqc)\n",
    "    model = tf.keras.Model(inputs=[input_tensor], outputs=Q_values)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = generate_model_Qlearning(qubits, n_layers, n_actions, observables, False)\n",
    "model_target = generate_model_Qlearning(qubits, n_layers, n_actions, observables, True)\n",
    "\n",
    "model_target.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "57TxgIN5wpUZ",
    "outputId": "da6a3aee-1722-40b3-fae5-2972d8c2ed88"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "jHp42R4twpUa",
    "outputId": "69a20e50-fde4-4634-897b-01edaeab420a"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model_target, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vricJOvXwpUa"
   },
   "source": [
    "You can now implement the deep Q-learning algorithm and test it on the CartPole-v1 environment. For the policy of the agent, you can use an $\\varepsilon$-greedy policy:\n",
    "$$ \\pi(a|s) =\n",
    "\\begin{cases}\n",
    "\\delta_{a,\\text{argmax}_{a'} Q_\\theta(s,a')}\\quad \\text{w.p.}\\quad 1 - \\varepsilon\\\\\n",
    "\\frac{1}{\\text{num_actions}}\\quad \\quad \\quad \\quad \\text{w.p.}\\quad \\varepsilon\n",
    "\\end{cases} $$\n",
    "where $\\varepsilon$ is multiplicatively decayed at each episode of interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMteuedswpUb"
   },
   "source": [
    "Start by defining a function that performs an interaction step in the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0L9cV26PwpUb"
   },
   "outputs": [],
   "source": [
    "def interact_env(state, model, epsilon, n_actions, env):\n",
    "    # Preprocess state\n",
    "    state_array = np.array(state) \n",
    "    state = tf.convert_to_tensor([state_array])\n",
    "\n",
    "    # Sample action\n",
    "    coin = np.random.random()\n",
    "    if coin > epsilon:\n",
    "        q_vals = model([state])\n",
    "        action = int(tf.argmax(q_vals[0]).numpy())\n",
    "    else:\n",
    "        action = np.random.choice(n_actions)\n",
    "\n",
    "    # Apply sampled action in the environment, receive reward and next state\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    interaction = {'state': state_array, 'action': action, 'next_state': next_state.copy(),\n",
    "                   'reward': reward, 'done':np.float32(done)}\n",
    "    \n",
    "    return interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDiw3iJywpUb"
   },
   "source": [
    "and a function that updates the Q-function using a batch of interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RR2DjesVwpUb"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def Q_learning_update(states, actions, rewards, next_states, done, model, gamma, n_actions):\n",
    "    states = tf.convert_to_tensor(states)\n",
    "    actions = tf.convert_to_tensor(actions)\n",
    "    rewards = tf.convert_to_tensor(rewards)\n",
    "    next_states = tf.convert_to_tensor(next_states)\n",
    "    done = tf.convert_to_tensor(done)\n",
    "\n",
    "    # Compute their target q_values and the masks on sampled actions\n",
    "    future_rewards = model_target([next_states])\n",
    "    target_q_values = rewards + (gamma * tf.reduce_max(future_rewards, axis=1)\n",
    "                                                   * (1.0 - done))\n",
    "    masks = tf.one_hot(actions, n_actions)\n",
    "\n",
    "    # Train the model on the states and target Q-values\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        q_values = model([states])\n",
    "        q_values_masked = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "        loss = tf.keras.losses.Huber()(target_q_values, q_values_masked)\n",
    "\n",
    "    # Backpropagation\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    for optimizer, w in zip([optimizer_in, optimizer_var, optimizer_out], [w_in, w_var, w_out]):\n",
    "        optimizer.apply_gradients([(grads[w], model.trainable_variables[w])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfXHhqaPwpUb"
   },
   "source": [
    "Define the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQ937aYPwpUc"
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "n_episodes = 2000\n",
    "\n",
    "# Define replay memory\n",
    "max_memory_length = 10000 # Maximum replay length\n",
    "replay_memory = deque(maxlen=max_memory_length)\n",
    "\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.01  # Minimum epsilon greedy parameter\n",
    "decay_epsilon = 0.99 # Decay rate of epsilon greedy parameter\n",
    "batch_size = 16\n",
    "steps_per_update = 10 # Train the model every x steps\n",
    "steps_per_target_update = 30 # Update the target model every x steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHsHnuHmwpUc"
   },
   "source": [
    "Prepare the optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "713nl3oUwpUc"
   },
   "outputs": [],
   "source": [
    "optimizer_in = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "optimizer_var = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "optimizer_out = tf.keras.optimizers.Adam(learning_rate=0.1, amsgrad=True)\n",
    "\n",
    "# Assign the model parameters to each optimizer\n",
    "w_in, w_var, w_out = 1, 0, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwE0buDowpUd"
   },
   "source": [
    "Now implement the main training loop of the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjjTamvywpUd"
   },
   "source": [
    "Note: This agent may need to simulate several million quantum circuits and can take as much as ~40 minutes to finish training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "er9fXHH_wpUd",
    "outputId": "b165066b-c239-4bf0-b444-7c710640241c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "    \n",
    "episode_reward_history = []\n",
    "step_count = 0\n",
    "for episode in range(n_episodes):\n",
    "    episode_reward = 0\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        # Interact with env\n",
    "        interaction = interact_env(state, model, epsilon, n_actions, env)\n",
    "        \n",
    "        # Store interaction in the replay memory\n",
    "        replay_memory.append(interaction)\n",
    "        \n",
    "        state = interaction['next_state']\n",
    "        episode_reward += interaction['reward']\n",
    "        step_count += 1\n",
    "        \n",
    "        # Update model\n",
    "        if step_count % steps_per_update == 0:\n",
    "            # Sample a batch of interactions and update Q_function\n",
    "            training_batch = np.random.choice(replay_memory, size=batch_size)\n",
    "            Q_learning_update(np.asarray([x['state'] for x in training_batch]),\n",
    "                              np.asarray([x['action'] for x in training_batch]),\n",
    "                              np.asarray([x['reward'] for x in training_batch], dtype=np.float32),\n",
    "                              np.asarray([x['next_state'] for x in training_batch]),\n",
    "                              np.asarray([x['done'] for x in training_batch], dtype=np.float32),\n",
    "                              model, gamma, n_actions)\n",
    "        \n",
    "        # Update target model\n",
    "        if step_count % steps_per_target_update == 0:\n",
    "            model_target.set_weights(model.get_weights())\n",
    "        \n",
    "        # Check if the episode is finished\n",
    "        if interaction['done']:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon * decay_epsilon, epsilon_min)\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if (episode+1)%10 == 0:\n",
    "        avg_rewards = np.mean(episode_reward_history[-10:])\n",
    "        print(\"Episode {}/{}, average last 10 rewards {}\".format(\n",
    "            episode+1, n_episodes, avg_rewards))\n",
    "        if avg_rewards >= 500.0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BG8BWOSYwpUd"
   },
   "source": [
    "Plot the learning history of the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "sSRMtk-swpUe",
    "outputId": "a2a4c5a8-92cf-495a-da9d-6be0e8f673a2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(episode_reward_history)\n",
    "plt.xlabel('Epsiode')\n",
    "plt.ylabel('Collected rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_7rJf0iwpUe"
   },
   "source": [
    "Similarly to the plot above, you should see that after ~1000 episodes, the performance of the agent gets close to optimal, i.e., 500 rewards per episode. Learning takes longer for Q-learning agents since the Q-function is a \"richer\" function to be learned than the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8X49f8owpUe"
   },
   "source": [
    "## 4. Exercise\n",
    "\n",
    "Now that you have trained two different types of models, try experimenting with different environments (and different numbers of qubits and layers). You could also try combining the PQC models of the last two sections into an [actor-critic agent](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#actor-critic)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "jxWGru_NwpUK",
    "_u3QBKbvwpUP",
    "X8X49f8owpUe"
   ],
   "name": "quantum_reinforcement_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
